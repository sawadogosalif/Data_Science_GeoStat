{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a786b407",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    " \n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import FloatType, StringType\n",
    "from pyspark.sql.window import Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea7bb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) General functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4c9d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def build_master_internal(cmd, loyalty, sales_features, coolers, sfe, red, data_extraction_date=\"2020-05-30\", id_cust=\"CUSTOMER_ID\",):\n",
    "  \"\"\"This function combines pre-processed internal data into a single master file on customer level.\n",
    " \n",
    "  Arguments:\n",
    "      cmd {pyspark.sql.DataFrame} -- A spark dataframe that contains cmd data on Marrakech city\n",
    "      loyalty {pyspark.sql.DataFrame} -- A spark dataframe that contains customer loyalty data\n",
    "      sales_features {pyspark.sql.DataFrame} -- A spark dataframe that contains engineered sales features, on customer level\n",
    "      coolers {pyspark.sql.DataFrame} -- A spark dataframe that contains cooler data aggregated to customer level\n",
    "      sfe {pyspark.sql.DataFrame} -- A spark dataframe that contains sfe data aggregated to customer level (period total)\n",
    "      red {pyspark.sql.DataFrame} -- A spark dataframe that contains red data aggregated to customer level (weekly mean)\n",
    "      data_extraction_date {str} -- String to indicate a common date of extraction - of the type \"2020-05-30\"\n",
    "      id_cust {str} -- Name of customer id variable to use\n",
    " \n",
    "  Returns:\n",
    "      pyspark.sql.DataFrame -- A spark dataframe that contains the combined internal data.\n",
    "  \"\"\"\n",
    "  \n",
    "  master = \\\n",
    "    cmd. \\\n",
    "    join(loyalty, on=[\"CUSTOMER_COD\", \"CUSTOMER_ID\"], how=\"left_outer\"). \\\n",
    "    join(sales_features, on=[id_cust], how=\"left_outer\"). \\\n",
    "    join(coolers_to_customer_count(coolers), on=[id_cust], how=\"left_outer\"). \\\n",
    "    join(sfe_to_customer_total(sfe, data_extraction_date), on=[id_cust], how=\"left_outer\"). \\\n",
    "    join(red_to_customer_total(red, data_extraction_date), on=[id_cust], how=\"left_outer\")\n",
    "  \n",
    "  return master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd4c8f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def expand_master_2(master_internal, taa, seasonal, hybrids, id_var=\"CUSTOMER_ID\"):\n",
    "  \"\"\"This function combines the internal master data with external sources - into a master file on customer level.\n",
    " \n",
    "  Arguments:\n",
    "      master_internal {pyspark.sql.DataFrame} -- A spark dataframe that contains the customer level internal master data \n",
    "      taa {pyspark.sql.DataFrame} -- A spark dataframe that contains customer level trade area analysis output \n",
    "      seasonality {pyspark.sql.DataFrame} -- A spark dataframe that contains seasonality features\n",
    "      hybrids {pyspark.sql.DataFrame} -- A spark dataframe that contains AG hybrid features\n",
    "      id_var {str} -- Name of customer id\n",
    " \n",
    "  Returns:\n",
    "      pyspark.sql.DataFrame -- A spark dataframe combining internal and external data on customer level.\n",
    "  \"\"\"\n",
    " \n",
    "  # add external features to the internal master set\n",
    "  master_expanded = \\\n",
    "    master_internal. \\\n",
    "    join(taa.withColumnRenamed(\"LATITUDE\", \"LATITUDE_TAA\").withColumnRenamed(\"LONGITUDE\", \"LONGITUDE_TAA\"), on=[id_var, \"CHANNEL_CUSTOM\"], how=\"left\"). \\\n",
    "    join(seasonal, on=id_var, how=\"left\"). \\\n",
    "    join(hybrids, on=id_var, how=\"left\")\n",
    "  \n",
    "  return master_expanded\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36eca40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_master(master_internal, ta_matched_geo, ta_matched_no_geo, poi_matched):\n",
    "  \"\"\"This function combines the internal master data with external sources - into a master file on customer level.\n",
    " \n",
    "  Arguments:\n",
    "      master_internal {pyspark.sql.DataFrame} -- A spark dataframe that contains the customer level internal master data \n",
    "      ta_matched_geo {pyspark.sql.DataFrame} -- A spark dataframe that contains customer level trip advisor features with included geo match\n",
    "      ta_matched_no_geo {pyspark.sql.DataFrame} -- A spark dataframe that contains customer level trip advisor features without geo match\n",
    "      poi_matched {pyspark.sql.DataFrame} -- A spark dataframe that contains customer level point of interest features \n",
    " \n",
    "  Returns:\n",
    "      pyspark.sql.DataFrame -- A spark dataframe combining internal and external data on customer level.\n",
    "  \"\"\"\n",
    "  \n",
    "  # prep external sources\n",
    "  \n",
    "  # cust id-s are repeated in the 2 sets - currently dropping repeats in non-geo matches, consider whether to preserve both matchings instead\n",
    "  droppers = ta_matched_no_geo.select(F.col(\"L_CUSTOMER_ID\")).join(ta_matched_geo.select(F.col(\"L_CUSTOMER_ID\")), how=\"inner\", on=[\"L_CUSTOMER_ID\"])\n",
    "  droppers = droppers.select(\"L_CUSTOMER_ID\").rdd.flatMap(lambda x: x).collect()\n",
    "  \n",
    "  ta_matched = stamp_feature_source(ta_matched_geo.union(ta_matched_no_geo.filter(~ta_matched_no_geo.L_CUSTOMER_ID.isin(droppers))), \"_MATCHED_TA\")\n",
    "  poi_matched = stamp_feature_source(poi_matched, \"_MATCHED_POI\")\n",
    " \n",
    "  # add external features to the internal master set\n",
    "  master_expanded = \\\n",
    "    master_internal. \\\n",
    "    join(ta_matched, master_internal.CUSTOMER_ID == ta_matched.L_CUSTOMER_ID_MATCHED_TA, how=\"left\"). \\\n",
    "    join(poi_matched, master_internal.CUSTOMER_ID == poi_matched.CUSTOMER_ID_MATCHED_POI, how=\"left\")\n",
    "  \n",
    "  return master_expanded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c21bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_from_junk_cols(data, thresh=0.8, regex=\"Age_Group_\"):\n",
    "  \"\"\"This function cleans a dataset from unusable columns - with no meaningful interpretation or too many missing values.\n",
    " \n",
    "  Arguments:\n",
    "      data {pyspark.sql.DataFrame} -- A spark dataframe to be cleaned up \n",
    "      thresh {float} -- Threshold value for null/nan count @ and above which to drop features (defaults to 0.8)\n",
    "      regex {float} -- Regular expression to use in excluding groups of variables by strings contained in their names \n",
    " \n",
    "  Returns:\n",
    "      pyspark.sql.DataFrame -- A cleaned spark dataframe.\n",
    "  \"\"\"\n",
    "  \n",
    "  df_nrows = data.count()\n",
    "  \n",
    "  # 1) Define columns with no meaningful interpretation - to be dropped\n",
    "  exclude = list(data.toPandas().filter(regex=regex).columns)\n",
    " \n",
    "  # 2) Define columns with null/nan count above acceptable threshold - to be dropped\n",
    "  missings = data.select([F.count(F.when(F.isnan(c) | F.col(c).isNull(), c)).alias(c) for c in data.columns])\n",
    "  drop = missings.withColumn(\"id\", F.lit(\"null_nan_count\")).toPandas().set_index(\"id\").T.reset_index().rename(columns={\"index\":\"variable\"})\n",
    "  drop = drop.loc[(drop[\"null_nan_count\"] >= df_nrows*thresh)][\"variable\"].tolist()\n",
    " \n",
    "  # 3) Clean data from 1) [exclude] and 2) [drop] columns\n",
    "  \n",
    "  return data.drop(*(drop+exclude))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99004a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stamp_feature_source(data, suffix):\n",
    "  \"\"\"This function takes a dataframe and adds a suffix to all column names.\n",
    " \n",
    "  Arguments:\n",
    "      data {pyspark.sql.DataFrame} -- A spark dataframe to operate on.\n",
    "      suffix {str} -- suffix to add to column names.\n",
    " \n",
    "  Returns:\n",
    "      pyspark.sql.DataFrame -- A spark dataframe with extended column names.\n",
    "  \"\"\"  \n",
    "  \n",
    "  return data.select([F.col(x).alias(x+suffix) for x in data.columns])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66e7073f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def derive_spark_week(spark_data, date_var=\"DATE_ISO\"):\n",
    "  \"\"\"This function takes the raw sales data and adds a week variable,\n",
    "  later to be used in aggregation manipulations.\n",
    " \n",
    "  Arguments:\n",
    "      spark_data {pyspark.sql.DataFrame} -- A spark dataframe to use\n",
    "      date_var {string} -- Name of date column to use in week derivation\n",
    " \n",
    "  Returns:\n",
    "      pyspark.sql.DataFrame -- A spark dataframe that contains the raw sales data and one additional\n",
    "          variable called week_index representing week numbers from earliest observed sale.\n",
    "  \"\"\"\n",
    "  \n",
    "  # 1) first collect all unique data\n",
    "  # 2) then put them in a python list\n",
    "  dataset_dates = \\\n",
    "    spark_data. \\\n",
    "    groupBy(date_var). \\\n",
    "    agg({date_var: \"count\"}). \\\n",
    "    select(date_var).sort(F.col(date_var).asc()). \\\n",
    "    collect()\n",
    " \n",
    "  unique_dates = [x[date_var] for x in dataset_dates]\n",
    " \n",
    "  start_date = datetime.datetime.strptime(str(min(unique_dates)), '%Y%m%d')\n",
    "  end_date = datetime.datetime.strptime(str(max(unique_dates)), '%Y%m%d')\n",
    " \n",
    "  print(\"The start date of the data is {} and it will be considered as week index 1, the end date is {} and will be considered the last week index\".\n",
    "        format(start_date.strftime('/%Y/%m/%d'), end_date.strftime('/%Y/%m/%d')))\n",
    " \n",
    "  date_range = [start_date + datetime.timedelta(days=x) for x in range((end_date - start_date).days+1)]\n",
    " \n",
    "  res = []\n",
    "  week_index = 0\n",
    " \n",
    "  # 3) derive week index\n",
    "  # 4) append it to original spark data\n",
    "  for each_possible_date in date_range:\n",
    " \n",
    "    if each_possible_date.weekday() == 0:\n",
    "        week_index = week_index + 1\n",
    " \n",
    "    temp_row = Row(\n",
    "      week_index=week_index, \n",
    "      DATE_ISO=each_possible_date.year*10000 + each_possible_date.month*100 + each_possible_date.day, \n",
    "      seasonal_week_index=each_possible_date.strftime(\"%V\")  # ISO 8601 week\n",
    "    ) \n",
    " \n",
    "    temp_row.__fields__ = [\"week_index\", date_var, \"seasonal_week_index\"]\n",
    "    res.append(temp_row)  \n",
    "    \n",
    "  return spark_data.join(spark.createDataFrame(res), on=\"DATE_ISO\", how=\"left\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de230cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_time_elapsed(data, time_spec, data_extraction_date):\n",
    "  \"\"\"This function adds customer level features on days elapsed since earliest and latest occurance as relevant for a given dataset.\n",
    " \n",
    "  Arguments:\n",
    "      data {pyspark.sql.DataFrame} -- A spark dataframe to establish temporal features for\n",
    "      time_spec {str} -- String to indicate dataset-specific spec (i.e. \"sfe_visit\", \"red_score\")\n",
    "      data_extraction_date {str} -- String to indicate a dataset's date of extraction - of the type \"2020-05-30\"\n",
    " \n",
    "  Returns:\n",
    "      pyspark.sql.DataFrame -- A spark dataframe expanded with time-elapsed-features.\n",
    "  \"\"\"\n",
    "  \n",
    "  data = data. \\\n",
    "    withColumn(\n",
    "      \"time_since_first_\" + time_spec + \"_days\", \n",
    "      F.datediff(F.to_date(F.lit(data_extraction_date)), F.to_date(F.col(\"date_customer_first_\" + time_spec).cast(StringType()), \"yyyyMMdd\"))\n",
    "    ). \\\n",
    "    withColumn(\n",
    "      \"time_since_last_\" + time_spec + \"_days\", \n",
    "      F.datediff(F.to_date(F.lit(data_extraction_date)), F.to_date(F.col(\"date_customer_last_\" + time_spec).cast(StringType()), \"yyyyMMdd\"))\n",
    "    )\n",
    "  \n",
    "  return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "859c3443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Cooler functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfd8ea29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def coolers_to_customer_count(coolers, id_cust=\"CUSTOMER_ID\"):\n",
    "  \"\"\"This function aggregates the cooler data to customer level.\n",
    " \n",
    "  Arguments:\n",
    "      coolers {pyspark.sql.DataFrame} -- A spark dataframe that contains the raw cooler data\n",
    "      id_cust {str} -- Name of customer id variable to use\n",
    " \n",
    "  Returns:\n",
    "      pyspark.sql.DataFrame -- A spark dataframe with cooler data aggregated to customer level.\n",
    "  \"\"\"\n",
    "  \n",
    "  # set specs\n",
    "  date_col = \"PLACEMENT_DATE\"\n",
    "  value_cols = [\"COOLER_COD\"]\n",
    "  \n",
    "  # adjust dummy names\n",
    "  door_types = coolers.select(F.col(\"NUM_DOORS\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "  door_types = [\"CUSTOMER_COUNT_COOLER_DOORS_\" + str(x).replace(\".\", \"_\") for x in door_types]  \n",
    "  \n",
    "  # perform init prep (helper flags)\n",
    "  coolers = cooler_init_prep(coolers)\n",
    "  \n",
    "  # aggregate to customer level\n",
    "  cool_cust = coolers. \\\n",
    "    groupBy(id_cust). \\\n",
    "    agg(*(\n",
    "      [F.count(x).alias(\"CUSTOMER_TOTAL_COUNT_COOLERS\") for x in coolers.columns if x in value_cols]\n",
    "      + [F.sum(x).alias(x) for x in coolers.columns if x in door_types] \n",
    "      + [F.min(date_col).alias(\"date_customer_first_cooler\"), F.max(date_col).alias(\"date_customer_last_cooler\")]  \n",
    "      + [F.sum(\"COOLER_EARLIEST_NUM_DOORS\").alias(\"CUSTOMER_EARLIEST_DATE_SUM_COOLER_DOORS\")]\n",
    "      + [F.sum(\"COOLER_LATEST_NUM_DOORS\").alias(\"CUSTOMER_LATEST_DATE_SUM_COOLER_DOORS\")]\n",
    "      + [F.count(\"CUSTOMER_EARLIEST_DATE_COOLER_COUNT\").alias(\"CUSTOMER_EARLIEST_DATE_COOLER_COUNT\")]\n",
    "      + [F.count(\"CUSTOMER_LATEST_DATE_COOLER_COUNT\").alias(\"CUSTOMER_LATEST_DATE_COOLER_COUNT\")]\n",
    "    ))\n",
    " \n",
    "  return cool_cust\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21de96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cooler_init_prep(coolers, id_cust=\"CUSTOMER_ID\"):\n",
    "  \"\"\"This function takes the raw cooler data and expands it with flag-like features \n",
    "  to be leveraged in the aggregation stage.\n",
    " \n",
    "  Arguments:\n",
    "      coolers {pyspark.sql.DataFrame} -- A spark dataframe that contains the raw cooler data\n",
    "      id_cust {str} -- Name of customer id variable to use\n",
    " \n",
    "  Returns:\n",
    "      pyspark.sql.DataFrame -- A spark dataframe with the expanded cooler data.\n",
    "  \"\"\"\n",
    "  \n",
    "  # set specs\n",
    "  date_col = \"PLACEMENT_DATE\"\n",
    "  value_cols = [\"COOLER_COD\"]\n",
    "  \n",
    "  # scope door dummies \n",
    "  door_types = coolers.select(F.col(\"NUM_DOORS\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "  types_expr = [F.when(F.col(\"NUM_DOORS\") == x, 1).otherwise(0).alias(\"CUSTOMER_COUNT_COOLER_DOORS_\" + str(x).replace(\".\", \"_\")) for x in door_types]\n",
    "  \n",
    "  # add dummies to original data\n",
    "  coolers = coolers.select(\"CUSTOMER_ID\", \"CUSTOMER_COD\", \"COMPANY_COD\", \"NUM_DOORS\", \"PLACEMENT_DATE\", \"COOLER_COD\", *types_expr)\n",
    "  \n",
    "  # add more flags to leverage in the aggregation step\n",
    "  w = (Window()\n",
    "    .partitionBy(coolers[id_cust])\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.unboundedFollowing))\n",
    " \n",
    "  coolers = coolers. \\\n",
    "    withColumn(\"sfe_latest_date\", F.max(coolers[date_col]).over(w)). \\\n",
    "    withColumn(\"sfe_earliest_date\", F.min(coolers[date_col]).over(w))\n",
    " \n",
    "  coolers = coolers. \\\n",
    "    withColumn(\"sfe_latest_flag\", F.when(coolers[\"sfe_latest_date\"] == coolers[date_col], F.lit(1)).otherwise(F.lit(0))). \\\n",
    "    withColumn(\"sfe_earliest_flag\", F.when(coolers[\"sfe_earliest_date\"] == coolers[date_col], F.lit(1)).otherwise(F.lit(0)))\n",
    "  \n",
    "  coolers = coolers. \\\n",
    "    withColumn(\"COOLER_EARLIEST_NUM_DOORS\", F.when(coolers[\"sfe_earliest_flag\"] == 1, coolers[\"NUM_DOORS\"])). \\\n",
    "    withColumn(\"COOLER_LATEST_NUM_DOORS\", F.when(coolers[\"sfe_latest_flag\"] == 1, coolers[\"NUM_DOORS\"])). \\\n",
    "    withColumn(\"CUSTOMER_EARLIEST_DATE_COOLER_COUNT\", F.when(coolers[\"sfe_earliest_flag\"] == 1, F.lit(1))). \\\n",
    "    withColumn(\"CUSTOMER_LATEST_DATE_COOLER_COUNT\", F.when(coolers[\"sfe_latest_flag\"] == 1, F.lit(1)))  \n",
    "  \n",
    "  return coolers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af7afa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) SFE functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2242c69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sfe_to_customer_total(sfe, data_extraction_date=\"2020-05-30\", id_cust=\"CUSTOMER_ID\",):\n",
    "  \"\"\"This function aggregates the sfe data to customer level.\n",
    " \n",
    "  Arguments:\n",
    "      sfe {pyspark.sql.DataFrame} -- A spark dataframe that contains the raw sfe data\n",
    "      data_extraction_date {str} -- String to indicate a dataset's date of extraction - of the type \"2020-05-30\"\n",
    "      id_cust {str} -- Name of customer id variable to use\n",
    " \n",
    "  Returns:\n",
    "      pyspark.sql.DataFrame -- A spark dataframe with sfe data aggregated to customer level.\n",
    "  \"\"\"\n",
    "  \n",
    "  # set specs\n",
    "  value_cols = [\"SCHEDULED\", \"SCANNED\", \"VISITED\", \"SALE\"]\n",
    "  time_spec = \"sfe_visit\"\n",
    "  \n",
    "  # aggregate to customer level\n",
    "  sfe_cust = sfe. \\\n",
    "    withColumn(\"YM\", F.col(\"VISIT_DATE\").substr(1, 6)). \\\n",
    "    groupBy(*([id_cust] + [\"YM\"])). \\\n",
    "    agg(*(\n",
    "      [F.sum(x).alias(x) for x in sfe.columns if x in value_cols]\n",
    "      + [F.min(F.col(\"VISIT_DATE\")).alias(\"date_customer_first_sfe_visit\"), \n",
    "         F.max(F.col(\"VISIT_DATE\")).alias(\"date_customer_last_sfe_visit\")] \n",
    "    )). \\\n",
    "    groupBy(id_cust). \\\n",
    "    agg(*(\n",
    "      [F.mean(x).alias(\"CUSTOMER_MONTHLY_MEAN_\" + x) for x in sfe.columns if x in value_cols]\n",
    "      + [F.min(F.col(\"date_customer_first_sfe_visit\")).alias(\"date_customer_first_sfe_visit\"), \n",
    "         F.max(F.col(\"date_customer_last_sfe_visit\")).alias(\"date_customer_last_sfe_visit\")] \n",
    "    ))\n",
    "   \n",
    "  # add time elapsed vars  \n",
    "  sfe_cust = get_time_elapsed(sfe_cust, time_spec=time_spec, data_extraction_date=data_extraction_date)\n",
    "  \n",
    "  return sfe_cust\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6d943da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) RED functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dbe181b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def red_to_customer_total(red, data_extraction_date=\"2020-05-30\", id_cust=\"CUSTOMER_ID\",):\n",
    "  \"\"\"This function aggregates the red data to customer level.\n",
    " \n",
    "  Arguments:\n",
    "      red {pyspark.sql.DataFrame} -- A spark dataframe that contains the raw red data\n",
    "      data_extraction_date {str} -- String to indicate a dataset's date of extraction - of the type \"2020-05-30\"\n",
    "      id_cust {str} -- Name of customer id variable to use\n",
    " \n",
    "  Returns:\n",
    "      pyspark.sql.DataFrame -- A spark dataframe with red data aggregated to customer level.\n",
    "  \"\"\"\n",
    "  \n",
    "  # starting specs\n",
    "  time_spec = \"red_score\"\n",
    "  \n",
    "  # create dummies to leverage in the aggregation\n",
    "  types = red.select(F.col(\"CRITERE_RED\")).distinct().rdd.flatMap(lambda x: x).collect()\n",
    "  types_scores = [F.when(F.col(\"CRITERE_RED\") == x, 1).otherwise(0).alias(\"RED_SCORE_FOR_\" + \"CRITERE_\" + x) for x in types]\n",
    "  types_scores_pond = [F.when(F.col(\"CRITERE_RED\") == x, 1).otherwise(0).alias(\"RED_SCORE_POND_FOR_\" + \"CRITERE_\" + x) for x in types]\n",
    "  \n",
    "  # column names\n",
    "  score_cols = [\"RED_SCORE_FOR_\" + \"CRITERE_\" + x for x in types]  \n",
    "  score_pond_cols = [\"RED_SCORE_POND_FOR_\" + \"CRITERE_\" + x for x in types]  \n",
    "  \n",
    "  # add week -> needed for mean weekly calculations\n",
    "  red = derive_spark_week(\n",
    "    red.select(*(red.columns + types_scores + types_scores_pond)), \n",
    "    date_var=\"DATE_ISO\")\n",
    "  \n",
    "  # populate dummies with meaningful data as per availability by red criteria\n",
    "  for each in score_cols:\n",
    "    red = red.withColumn(each, F.col(\"SCORE_RED\") * F.col(each))\n",
    " \n",
    "  for each in score_pond_cols:\n",
    "    red = red.withColumn(each, F.col(\"RED_SCORE_POND\") * F.col(each))\n",
    " \n",
    "  # aggregate by customer + week\n",
    "  red_cust = red. \\\n",
    "    groupBy(id_cust, \"week_index\"). \\\n",
    "    agg(*(\n",
    "      [F.mean(x).alias(x) for x in red.columns if x in score_cols+score_pond_cols]\n",
    "      + [F.min(F.col(\"DATE_ISO\")).alias(\"date_customer_first_red_score\"), \n",
    "         F.max(F.col(\"DATE_ISO\")).alias(\"date_customer_last_red_score\")]  \n",
    "    ))\n",
    " \n",
    "  # aggregate by customer only\n",
    "  red_cust = red_cust. \\\n",
    "    groupBy(id_cust). \\\n",
    "    agg(*(\n",
    "      [F.mean(x).alias(\"MEAN_WEEKLY_\"+x) for x in red_cust.columns if x in score_cols+score_pond_cols]\n",
    "      + [F.min(F.col(\"date_customer_first_red_score\")).alias(\"date_customer_first_red_score\"), \n",
    "         F.max(F.col(\"date_customer_last_red_score\")).alias(\"date_customer_last_red_score\")]  \n",
    "    ))\n",
    "  \n",
    "  # add time elapsed vars  \n",
    "  red_cust = get_time_elapsed(red_cust, time_spec=time_spec, data_extraction_date=data_extraction_date)\n",
    "  \n",
    "  return red_cust"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PREMIER",
   "language": "python",
   "name": "premier"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
